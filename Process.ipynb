{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements and download Spacy small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 1)) (3.7.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement Tkinter (from versions: none)\n",
      "ERROR: No matching distribution found for Tkinter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: openai in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 4)) (4.34.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 5)) (2.14.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 6)) (4.66.1)\n",
      "Requirement already satisfied: progressbar2 in c:\\users\\kmmf8\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from -r requirements.txt (line 7)) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"en_core_web_sm\"\n",
    "\n",
    "# Check if the model is installed\n",
    "if model_name not in spacy.util.get_installed_models():\n",
    "    print(f\"Downloading and installing the '{model_name}' model...\")\n",
    "    \n",
    "    # Download and install the model\n",
    "    !python -m spacy download {model_name}\n",
    "else:\n",
    "    print(f\"The '{model_name}' model is already installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Define the folder path where the databases will be stored\n",
    "folder_path = \"databases\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Database names\n",
    "database_names = [\"TrainData.db\", \"UnprocessedData.db\"]\n",
    "\n",
    "# Loop to create and set up the databases\n",
    "for database_name in database_names:\n",
    "    database_path = os.path.join(folder_path, database_name)\n",
    "\n",
    "    # Check if the database file already exists\n",
    "    if not os.path.exists(database_path):\n",
    "        # Connect to the SQLite database (or create it if it doesn't exist)\n",
    "        connection = sqlite3.connect(database_path)\n",
    "\n",
    "        # Create a cursor object to execute SQL commands\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Create a table for each database (you can define the table schema as needed)\n",
    "        cursor.execute('''CREATE TABLE conversations (\n",
    "                            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                            user TEXT,\n",
    "                            message TEXT\n",
    "                         )''')\n",
    "\n",
    "        # Commit the changes and close the connection\n",
    "        connection.commit()\n",
    "        connection.close()\n",
    "\n",
    "        print(f\"Database '{database_name}' created in the '{folder_path}' folder.\")\n",
    "    else:\n",
    "        print(f\"Database '{database_name}' already exists in the '{folder_path}' folder. Skipping creation.\")\n",
    "\n",
    "print(\"All databases created or skipped if they already exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes all Data in the Data Folder and puts it in the Unprocessed Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting Data:   0%|          | 0/3239027 [00:00<?, ? row/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\Coding Projects\\LLM-Model-Trainer\\Process.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39mparquet_data\u001b[39m.\u001b[39mnum_rows, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m row\u001b[39m\u001b[39m\"\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInserting Data\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# Iterate through the rows of the Parquet data and insert into the database\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m parquet_data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39m# Specify the correct column names\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         \u001b[39mid\u001b[39m, system_prompt, question, response \u001b[39m=\u001b[39m row\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39m# Insert the row's values into the database\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Coding%20Projects/LLM-Model-Trainer/Process.ipynb#X12sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         insert_query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mINSERT INTO text_data (id, system_prompt, question, response) VALUES (?, ?, ?, ?)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the Parquet file\n",
    "parquet_file = \"Data/data.parquet\"\n",
    "\n",
    "# Connect to the \"UnprocessedData.db\" database\n",
    "db_connection = sqlite3.connect(\"databases/UnprocessedData.db\")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Read data from the Parquet file\n",
    "parquet_data = pq.read_table(parquet_file)\n",
    "\n",
    "# Check if the table already exists\n",
    "table_exists_query = \"SELECT name FROM sqlite_master WHERE type='table' AND name='text_data';\"\n",
    "cursor.execute(table_exists_query)\n",
    "table_exists = cursor.fetchone()\n",
    "\n",
    "if not table_exists:\n",
    "    # If the table doesn't exist, create it dynamically based on the Parquet schema\n",
    "    parquet_schema = parquet_data.schema\n",
    "    column_names = [field.name for field in parquet_schema]\n",
    "    column_definitions = ', '.join(['{} TEXT'.format(name) for name in column_names])\n",
    "    create_table_query = f'CREATE TABLE IF NOT EXISTS text_data ({column_definitions})'\n",
    "    cursor.execute(create_table_query)\n",
    "    db_connection.commit()\n",
    "\n",
    "# Use tqdm for the progress bar\n",
    "with tqdm(total=parquet_data.num_rows, unit=\" row\", desc=\"Inserting Data\") as pbar:\n",
    "    # Iterate through the rows of the Parquet data and insert into the database\n",
    "    for row in parquet_data:\n",
    "        # Specify the correct column names\n",
    "        id, system_prompt, question, response = row\n",
    "\n",
    "        # Insert the row's values into the database\n",
    "        insert_query = \"INSERT INTO text_data (id, system_prompt, question, response) VALUES (?, ?, ?, ?)\"\n",
    "        cursor.execute(insert_query, (id, system_prompt, question, response))\n",
    "        db_connection.commit()\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Close the database connection\n",
    "db_connection.close()\n",
    "\n",
    "print(\"Data from the Parquet file has been inserted into the 'UnprocessedData.db' database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Function to perform text preprocessing\n",
    "def preprocess_text(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "    stopwords_list = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to create the \"text_data\" table in the database\n",
    "def create_text_data_table(db_path):\n",
    "    connection = sqlite3.connect(db_path)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS text_data (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        content TEXT\n",
    "                     )''')\n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "# Function to process and move data from UnprocessedData to TrainData\n",
    "def process_and_move_data(unprocessed_db_path, train_db_path):\n",
    "    create_text_data_table(train_db_path)  # Create the table if it doesn't exist\n",
    "\n",
    "    # Connect to the \"UnprocessedData.db\" database\n",
    "    unprocessed_db_connection = sqlite3.connect(unprocessed_db_path)\n",
    "    unprocessed_cursor = unprocessed_db_connection.cursor()\n",
    "\n",
    "    # Connect to the \"TrainData.db\" database\n",
    "    train_db_connection = sqlite3.connect(train_db_path)\n",
    "    train_cursor = train_db_connection.cursor()\n",
    "\n",
    "    # Retrieve unprocessed data from the \"UnprocessedData\" database\n",
    "    unprocessed_cursor.execute(\"SELECT id, content FROM text_data\")\n",
    "    unprocessed_rows = unprocessed_cursor.fetchall()\n",
    "\n",
    "    # Create a spaCy NLP object for tokenization\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the data and insert into the \"TrainData\" database\n",
    "    for row in unprocessed_rows:\n",
    "        id, content = row\n",
    "        preprocessed_text = preprocess_text(content, nlp)\n",
    "        train_cursor.execute(\"INSERT INTO text_data (content) VALUES (?)\", (preprocessed_text,))\n",
    "        train_db_connection.commit()\n",
    "\n",
    "    # Close the database connections\n",
    "    unprocessed_db_connection.close()\n",
    "    train_db_connection.close()\n",
    "\n",
    "    print(\"Data processed and inserted into the 'TrainData.db' database.\")\n",
    "\n",
    "# Path to the \"UnprocessedData.db\" and \"TrainData.db\" databases\n",
    "unprocessed_db_path = \"databases/UnprocessedData.db\"\n",
    "train_db_path = \"databases/TrainData.db\"\n",
    "\n",
    "# Call the function to process and move data\n",
    "process_and_move_data(unprocessed_db_path, train_db_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
